#!/bin/bash

# DESCRIPTION:
#   Get all tags of said docker image.
#   Before execution, it checks if curl/wget and python are available.
#   To use this without "bash", change the firstline to "#!/bin/sh" and the set line to "set -eu".
# USAGE:
#   docker-tags <image-name>
# BASED ON:
#   https://stackoverflow.com/a/48931329
# TODO:
#   - Adapt to work with images with slash in name (e.g. bitnami/postgres)
#   - Add usage section in script
#   - Add option to bring all tags instead of just unique ones (e.g.
#     https://hub.docker.com/_/python#:~:text=the%20FAQ.)-,Simple%20Tags)
#   - Optimize python code readability
#   - Create test cases
#   - Adapt to not use python (perhaps)

set -euo pipefail

name=$1
# Initial URL
url=https://registry.hub.docker.com/v2/repositories/library/$name/tags/?page_size=100

# Detects if "curl" or "wget" are available
if command -v curl &>/dev/null; then
  download_cmd='curl -s'
elif command -v wget &>/dev/null; then
  download_cmd='wget -qO-'
else
  echo "Curl or wget is not installed!"
  exit 1
fi

# Detects if "python" is available and which version
if command -v python3 &>/dev/null; then
  py_bin=$(command -v python3)
elif command -v python &>/dev/null; then
  py_bin=$(command -v python)
else
  echo "Python is not installed!"
  exit 1
fi

(
  # Keep looping until the variable URL is empty
  while [ ! -z $url ]; do
    # Every iteration of the loop prints out a single dot to show progress as it got through all the pages (this is inline dot)
    >&2 echo -n "."
    # Curl the URL and pipe the output to Python. Python will parse the JSON and print the very first line as the next URL (it will leave it blank if there are no more pages)
    # then continue to loop over the results extracting only the name; all will be stored in a variable called content
    content=$(
      $download_cmd $url |
      $py_bin -c 'import sys, json; data = json.load(sys.stdin); print(data.get("next", "") or ""); print("\n".join([x["name"] for x in data["results"]]))'
    )
    # Let's get the first line of content which contains the next URL for the loop to continue
    url=$(echo "$content" | head -n 1)
    # Print the content without the first line (yes +2 is counter intuitive)
    echo "$content" | tail -n +2
  done;
  # Finally break the line of dots
  >&2 echo
) | cut -d '-' -f 1 | sort -V | uniq;

exit 0
